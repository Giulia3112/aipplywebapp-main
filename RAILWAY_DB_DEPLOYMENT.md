# ðŸš€ Railway Database Deployment - Step by Step

## âœ… Files Updated (Using Your Schema)

All files now use your existing `Opportunity` table schema:
- âœ… `database_setup.py` - Database connection + your Opportunity model
- âœ… `services/db_service.py` - Search, store, and filter logic
- âœ… `main_with_db.py` - FastAPI endpoints with database
- âœ… `requirements_db.txt` - All dependencies
- âœ… `create_tables.py` - Helper script to create tables

---

## ðŸ“‹ Deployment Steps

### **Step 1: Add PostgreSQL to Railway**

1. Go to Railway dashboard: https://railway.app
2. Open your `AIpply-AI-Agent-Main` project
3. Click **"+ New"**
4. Select **"Database"** â†’ **"PostgreSQL"**
5. Railway automatically creates `DATABASE_URL` environment variable
6. âœ… Done! PostgreSQL is ready

---

### **Step 2: Copy Files to Railway Repository**

Copy these files from **this folder** to your **Railway API repository**:

```
ðŸ“ Your Railway Repo Root:
  â”œâ”€â”€ database_setup.py          â† NEW (copy this)
  â”œâ”€â”€ create_tables.py            â† NEW (copy this)
  â”œâ”€â”€ main.py                     â† REPLACE with main_with_db.py
  â”œâ”€â”€ requirements.txt            â† UPDATE (merge requirements_db.txt)
  â””â”€â”€ services/
      â”œâ”€â”€ db_service.py           â† NEW (copy this)
      â””â”€â”€ run_scraper.py          (your existing file)
```

**Commands** (if using Git):
```bash
# Navigate to your Railway API folder
cd /path/to/AIpply-AI-Agent-Main

# Copy files
cp "path/to/database_setup.py" .
cp "path/to/create_tables.py" .
cp "path/to/main_with_db.py" main.py
cp "path/to/services/db_service.py" services/

# Update requirements.txt (add these lines):
cat requirements_db.txt >> requirements.txt

# Commit and push
git add .
git commit -m "Add PostgreSQL database integration"
git push
```

---

### **Step 3: Verify Railway Environment**

In Railway dashboard, check environment variables:
- âœ… `DATABASE_URL` - Auto-created by PostgreSQL service
- âœ… `PORT` - Set to `8000`
- âœ… `OPENAI_API_KEY` - Your existing API key
- âœ… Any other scraper-related variables

---

### **Step 4: Update requirements.txt**

Make sure your `requirements.txt` includes:
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
python-dateutil==2.8.2
requests==2.31.0
beautifulsoup4==4.12.2
lxml==4.9.3
```

Plus any other dependencies your scraper needs.

---

### **Step 5: Deploy to Railway**

Railway will automatically:
1. Detect changes
2. Rebuild your service
3. Create database tables on startup (via `init_db()`)
4. Start the API

**Monitor the logs:**
```
âœ… Database initialized successfully!
INFO:     Started server process
INFO:     Uvicorn running on http://0.0.0.0:8000
```

---

### **Step 6: Initialize Database with Data**

Once deployed, populate the database:

**Option A: Manual Refresh (Recommended)**
```bash
curl -X POST "https://aipply-ai-agent-main-production.up.railway.app/api/admin/refresh"
```

This will:
- Run your scraper
- Store all opportunities in database
- Return count of new opportunities

**Option B: Wait for First Search**
The API will auto-populate on the first search request.

---

## ðŸ§ª Testing

### **Test 1: Health Check**
```bash
curl https://aipply-ai-agent-main-production.up.railway.app/
```
Expected response:
```json
{
  "status": "healthy",
  "service": "AIpply Opportunity Search API",
  "version": "2.0 with Database"
}
```

### **Test 2: Search with Keyword** (This should FILTER now!)
```bash
curl "https://aipply-ai-agent-main-production.up.railway.app/search?keyword=machine+learning"
```
Expected: **Only** opportunities matching "machine learning"

### **Test 3: Get All Opportunities**
```bash
curl https://aipply-ai-agent-main-production.up.railway.app/api/admin/opportunities
```
Expected: All stored opportunities

---

## ðŸŽ¯ How the New System Works

### **Search Flow:**
```
User searches "AI scholarship" 
    â†“
API checks database for recent data (<6 hours old)
    â†“
If data exists: Search database (INSTANT! âš¡)
    â†“
Filter by keyword: title, description, type, location, tags
    â†“
Return filtered results (not all 57!)
    â†“
If data is stale: Trigger background refresh
```

### **Key Features:**
- âœ… **Fast searches** - Database queries are instant
- âœ… **Smart filtering** - AND logic (all keywords must match)
- âœ… **Auto-refresh** - Updates every 6 hours automatically
- âœ… **No duplicates** - URL is unique constraint
- âœ… **Fallback** - If DB fails, falls back to direct scraping

---

## ðŸ”§ Table Schema (Your Existing Model)

```python
opportunities
â”œâ”€â”€ id (Integer, Primary Key)
â”œâ”€â”€ title (String, Indexed)
â”œâ”€â”€ organization (String, Indexed)
â”œâ”€â”€ type (String, Indexed)          # scholarship, fellowship, grant
â”œâ”€â”€ deadline (Date)
â”œâ”€â”€ location (String, Indexed)
â”œâ”€â”€ description (Text)
â”œâ”€â”€ url (String, Unique)            # Prevents duplicates
â”œâ”€â”€ tags (String)
â”œâ”€â”€ is_verified (Boolean)
â”œâ”€â”€ created_at (DateTime)
â”œâ”€â”€ updated_at (DateTime)
```

---

## ðŸ› Troubleshooting

### **Issue: "Module not found: database_setup"**
- Make sure `database_setup.py` is in the root directory
- Check Railway logs for import errors

### **Issue: "No opportunities returned"**
```bash
# Check if database has data
curl https://aipply-ai-agent-main-production.up.railway.app/api/admin/opportunities

# If empty, manually refresh
curl -X POST https://aipply-ai-agent-main-production.up.railway.app/api/admin/refresh
```

### **Issue: "Database connection error"**
- Verify `DATABASE_URL` exists in Railway environment
- Check PostgreSQL service is running
- Railway auto-fixes URL format (postgres:// â†’ postgresql://)

### **Issue: "Still returning all results"**
- Check Railway logs for database query logs
- Verify `search_opportunities_db` is being called
- Test with browser console to see actual API response

---

## ðŸŽŠ Success Indicators

You'll know it's working when:
- âœ… `/search?keyword=AI` returns **only AI-related** opportunities
- âœ… Search is **instant** (no 5-10 second scraping delay)
- âœ… Same search twice is cached (super fast second time)
- âœ… Different keywords return **different** filtered results

---

## ðŸ“ž Next Steps After Deployment

1. **Test the search** on your frontend (localhost:5173/search-demo)
2. **Monitor Railway logs** for any errors
3. **Verify filtering** works (search "scholarship" vs "fellowship")
4. **Set up periodic refresh** (optional - already auto-refreshes every 6 hours)

---

Need help? Check Railway logs or test endpoints with `curl`! ðŸš€

